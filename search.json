[
  {
    "objectID": "models/supp_declutr.html",
    "href": "models/supp_declutr.html",
    "title": "DeCLUTR",
    "section": "",
    "text": "# https://github.com/JohnGiorgi/DeCLUTR\n\n!pip install git+https://github.com/JohnGiorgi/DeCLUTR.git --quiet\nFinally, let’s check to see if we have a GPU available, which we can use to dramatically speed up the embedding of text"
  },
  {
    "objectID": "models/supp_declutr.html#as-a-library",
    "href": "models/supp_declutr.html#as-a-library",
    "title": "DeCLUTR",
    "section": "1️⃣ As a library",
    "text": "1️⃣ As a library\nTo use the model as a library, import Encoder and pass it some text (it accepts both strings and lists of strings)\n\nfrom declutr import Encoder\nimport numpy as np\n# This can be a path on disk to a model you have trained yourself OR\n# the name of one of our pretrained models.\npretrained_model_or_path = \"declutr-small\"\n\n\nencoder = Encoder(pretrained_model_or_path, cuda_device=cuda_device)\n\n\n# downloading the model for declutr bas\nencoder_base = Encoder(\"declutr-base\", cuda_device = cuda_device)\n\n\ndef get_sentenes(fname):\n    with open(fname, 'r') as f:\n        all = f.read()\n        f.close()\n    name = fname[:-4]\n    all_sentences = [line for line in all.split('\\n') if len(line) > 0]\n    print('{} contains {} sentences'.format(name, len(all_sentences)))\n    return all_sentences\n\n# functions to \ndef small_embeddings(fname):\n    embeddings = encoder(get_sentenes(fname), batch_size=32)\n    temp = fname[:-4]+'_dcltr_sm.npy'\n    np.save(temp, embeddings)\n\n\ndef base_embeddings(fname):\n    embeddings = encoder_base(get_sentenes(fname), batch_size=32)\n    temp = fname[:-4]+'_dcltr_base.npy'\n    np.save(temp, embeddings)\n\n\nfname = 'a christmas carol_cleaned.txt'\nsmall_embeddings(fname)\nbase_embeddings(fname)\n\nfname = 'metamorphosis_cleaned.txt'\nsmall_embeddings(fname)\nbase_embeddings(fname)\n\nfname = 'heart of darkness_cleaned.txt'\nsmall_embeddings(fname)\nbase_embeddings(fname)\n\nfname = 'the prophet_cleaned.txt'\nsmall_embeddings(fname)\nbase_embeddings(fname)\n\na christmas carol_cleaned contains 1942 sentences\na christmas carol_cleaned contains 1942 sentences\nmetamorphosis_cleaned contains 795 sentences\nmetamorphosis_cleaned contains 795 sentences\nheart of darkness_cleaned contains 2430 sentences\nheart of darkness_cleaned contains 2430 sentences\nthe prophet_cleaned contains 647 sentences\nthe prophet_cleaned contains 647 sentences"
  },
  {
    "objectID": "models/supp_use.html",
    "href": "models/supp_use.html",
    "title": "use",
    "section": "",
    "text": "# download the USE sentence encoder from tensorflow hub\n# from the paper  (Cer et al.) Universal Sentence Encoder. arXiv:1803.11175, 2018.\nimport tensorflow_hub as hub\nembed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\n\n# import necessary libraries\nimport numpy as np\n\n# function to get sentences from the cleaned txt files\ndef get_sentenes(fname):\n    with open(fname, 'r') as f:\n        all = f.read()\n        f.close()\n    name = fname[:-4]\n    all_sentences = [line for line in all.split('\\n') if len(line) > 0]\n    print('{} contains {} sentences'.format(name, len(all_sentences)))\n    return all_sentences\n\n# function to generate embeddings using the USE\n# function saves the generated embeddings as a .npy array for future use\ndef use_embed(fname):\n    sent = get_sentenes(fname)\n    final = np.asarray(embed(sent))\n    np.save(fname[:-4]+'_use.npy',final)\n\n\n# generate embeddings for all the datasets, make sure the .txt files are in the correct directory\n\nfname = 'a christmas carol_cleaned.txt'\nuse_embed(fname)\n\nfname = 'heart of darkness_cleaned.txt'\nuse_embed(fname)\n\nfname = 'metamorphosis_cleaned.txt'\nuse_embed(fname)\n\nfname = 'the prophet_cleaned.txt'\nuse_embed(fname)\n\na christmas carol_cleaned contains 1942 sentences\nheart of darkness_cleaned contains 2430 sentences\nmetamorphosis_cleaned contains 795 sentences\nthe prophet_cleaned contains 647 sentences"
  },
  {
    "objectID": "models/supp_distil_roberta.html",
    "href": "models/supp_distil_roberta.html",
    "title": "distil-roberta",
    "section": "",
    "text": "# download the sentence transformers library\n!pip install -U sentence-transformers --quiet\n\n     |████████████████████████████████| 81kB 6.3MB/s \n     |████████████████████████████████| 2.3MB 12.5MB/s \n     |████████████████████████████████| 1.2MB 46.6MB/s \n     |████████████████████████████████| 3.3MB 54.4MB/s \n     |████████████████████████████████| 901kB 42.6MB/s \n  Building wheel for sentence-transformers (setup.py) ... done\n\n\n\n# download the encoder for distilbert and load the encoder on gpu, if available\nfrom sentence_transformers import SentenceTransformer\nmodel_distil = SentenceTransformer('distilbert-base-nli-mean-tokens', device='cuda')\n\n# use this if GPU is unavailable\n# model_distil = SentenceTransformer('distilbert-base-nli-mean-tokens')\n\n\n\n\n\n\n\n\n# download the encoder for RoBERTa\nmodel_robeta = SentenceTransformer('roberta-large-nli-stsb-mean-tokens', device='cuda')\n\n# use this if GPU is unavailable\n# model_robeta = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')\n\n\n\n\n\n\n\n\n# import necessary libraries\nimport numpy as np\n\n# function to get sentences from the cleaned txt files\ndef get_sentenes(fname):\n    with open(fname, 'r') as f:\n        all = f.read()\n        f.close()\n    name = fname[:-4]\n    all_sentences = [line for line in all.split('\\n') if len(line) > 0]\n    print('{} contains {} sentences'.format(name, len(all_sentences)))\n    return all_sentences\n\n# function to generate embeddings for distilbert and roberta, \n# save them as .npy arrays\ndef generate(fname):\n    sent = get_sentenes(fname)\n    final = np.asarray(model_distil.encode(sent))\n    np.save(fname[:-4]+'_distil.npy',final)\n\n    f2 = np.asarray(model_robeta.encode(sent))\n    np.save(fname[:-4]+'_roberta.npy',f2)\n\n\n# generate embeddings for all the datasets, make sure the .txt files are in the correct directory\n\nfname = 'a christmas carol_cleaned.txt'\ngenerate(fname)\n\nfname = 'heart of darkness_cleaned.txt'\ngenerate(fname)\n\nfname = 'metamorphosis_cleaned.txt'\ngenerate(fname)\n\nfname = 'the prophet_cleaned.txt'\ngenerate(fname)\n\na christmas carol_cleaned contains 1942 sentences\nheart of darkness_cleaned contains 2430 sentences\nmetamorphosis_cleaned contains 795 sentences\nthe prophet_cleaned contains 647 sentences"
  },
  {
    "objectID": "models/supp_scatter.html",
    "href": "models/supp_scatter.html",
    "title": "scatter",
    "section": "",
    "text": "import os\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\ndf = pd.read_csv('A Christmas Carol.csv')\n\n\ndf2 = df[:7].T\n\n\ndf2.columns = df2.iloc[0]\n\n\ndf2.drop(index = 'Unnamed: 0', inplace=True)\n\n\nfig, ax = plt.subplots(3,2, sharex=True, sharey = True, figsize = (10,8))\nax[0][0].tick_params(\n    axis='x',          # changes apply to the x-axis\n    which='major',      # both major and minor ticks are affected\n    bottom='off',      # ticks along the bottom edge are off\n    # top='off',         # ticks along the top edge are off\n)\nax[0][0].scatter(df2['USE'], df2['DeCLUTR Base'], s=0.2)\n# ax[0][0].set_xticks([])\nax[0][0].set_xlabel('USE')\nax[0][0].set_ylabel('DeCLUTR Base')\n\nax[0][1].scatter(df2['USE'], df2['DeCLUTR Small'], s=0.2)\nax[0][1].set_xlabel('USE')\nax[0][1].set_ylabel('DeCLUTR Small')\n\nax[1][0].scatter(df2['USE'], df2['InferSent FastText'], s=0.2)\nax[1][0].set_xlabel('USE')\nax[1][0].set_ylabel('InferSent FastText')\nax[1][0].tick_params(axis='x', bottom = 'off')\n\n\nax[1][1].scatter(df2['USE'], df2['InferSent GloVe'], s=0.2)\nax[1][1].set_xlabel('USE')\nax[1][1].set_ylabel('InferSent GloVe')\n\n\nax[2][0].scatter(df2['USE'], df2['DistilBERT'], s=0.2)\nax[2][0].set_xlabel('USE')\nax[2][0].set_ylabel('DistilBERT')\n\n\nax[2][1].scatter(df2['USE'], df2['RoBERTa'], s=0.2)\nax[2][1].set_xlabel('USE')\nax[2][1].set_ylabel('RoBERTa')\nplt.savefig('2.png', dpi = 300, bbox_inches='tight')\n# plt.scatter(x = df2['USE'], y=df2['DeCLUTR Base'], )"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "papers",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "papers",
    "section": "Install",
    "text": "Install\npip install papers"
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "papers",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2"
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()"
  },
  {
    "objectID": "ACL/acl paper plots.html",
    "href": "ACL/acl paper plots.html",
    "title": "Notebook to generate plots for the ACL Paper",
    "section": "",
    "text": "!pip install -Uqq fastcore\n\n     |█████▊                          | 10 kB 19.2 MB/s eta 0:00:01     |███████████▌                    | 20 kB 11.3 MB/s eta 0:00:01     |█████████████████▎              | 30 kB 8.9 MB/s eta 0:00:01     |███████████████████████         | 40 kB 8.2 MB/s eta 0:00:01     |████████████████████████████▉   | 51 kB 4.4 MB/s eta 0:00:01     |████████████████████████████████| 56 kB 2.4 MB/s\nOne can change the path of the folder containing these embeddings, so in this case, it would be A Christmas Carol, Heart of Darkness, Metamorphosis and The Prophet.\nThe additional number in the beginning is only added to sort the files in the required order."
  },
  {
    "objectID": "ACL/acl paper plots.html#scatter-plots",
    "href": "ACL/acl paper plots.html#scatter-plots",
    "title": "Notebook to generate plots for the ACL Paper",
    "section": "Scatter plots",
    "text": "Scatter plots\n\nchristmas carol\n\ndf.columns\n\nIndex(['DeCLUTR Base', 'DeCLUTR Small', 'DistilBERT', 'InferSent FastText',\n       'InferSent GloVe', 'Lexical Weights', 'RoBERTa', 'USE'],\n      dtype='object')\n\n\n\ndf[df.columns[0]]\n\n0       0.434782\n1       0.345723\n2       0.486630\n3       0.518739\n4       0.230040\n          ...   \n1936    0.511335\n1937    0.545136\n1938    0.486823\n1939    0.329762\n1940    0.684132\nName: DeCLUTR Base, Length: 1941, dtype: float64\n\n\n\nfig, ax = plt.subplots(4, 2, figsize=(5.5,7), sharex=True)\nk = 0\nfor row in range(4):\n    for col in range(2):\n        # ax[row][col].set_title(df.columns[k])\n        y = zscore(normalize(df[organized_labels[k]]))\n        x = zscore(normalize(df['RoBERTa']))\n        sns.scatterplot(y=y, x=x , ax=ax[row][col], s=2, marker='o', edgecolor=None)\n        ax[row][col].set_ylabel(label2[k])\n        ax[row][col].set_xlabel('RB')\n        ax[row][col].set_ylim([-4,  6])\n        ax[row][col].set_xlim([-6,  6])\n        k += 1\nplt.tight_layout()\nplt.savefig('scatter8.pdf', dpi=300, bbox_inches='tight')\n\n\n\n\n\ndata = pickle.load(open('a christmas carol_whole.pkl', 'rb'))\ndf = pd.DataFrame(data[0])\ndf = df[organized_labels]\ndf2 = pd.DataFrame(zscore(df, axis=0), columns=organized_labels)\ng = sns.PairGrid(df2)\ng.map(sns.scatterplot, s=2, marker='o', edgecolor=None)\n# plt.savefig('cc-scatter.pdf', dpi=300, bbox_inches='tight')\nplt.savefig('cc-scatter.png', dpi=300, bbox_inches='tight')\n\n\n\n\n\n\nmetamorphosis\n\ndata = pickle.load(open('metamorphosis_whole.pkl', 'rb'))\ndf = pd.DataFrame(data[0])\ndf = df[organized_labels]\ndf2 = pd.DataFrame(zscore(df, axis=0), columns=organized_labels)\ng = sns.PairGrid(df2)\ng.map(sns.scatterplot, s=2, marker='o', edgecolor=None)\n# plt.savefig('meta-scatter.pdf', dpi=300, bbox_inches='tight')\nplt.savefig('meta-scatter.png', dpi=300, bbox_inches='tight')"
  },
  {
    "objectID": "ACL/acl paper plots.html#heart-of-darkness",
    "href": "ACL/acl paper plots.html#heart-of-darkness",
    "title": "Notebook to generate plots for the ACL Paper",
    "section": "heart of darkness",
    "text": "heart of darkness\n\ndata = pickle.load(open('heart of darkness_whole.pkl', 'rb'))\ndf = pd.DataFrame(data[0])\ndf = df[organized_labels]\ndf2 = pd.DataFrame(zscore(df, axis=0), columns=organized_labels)\ng = sns.PairGrid(df2)\ng.map(sns.scatterplot, s=2, marker='o', edgecolor=None)\n# plt.savefig('hod-scatter.pdf', dpi=300, bbox_inches='tight')\nplt.savefig('hod-scatter.png', dpi=300, bbox_inches='tight')\n\n\n\n\n\nprophet\n\ndata = pickle.load(open('the prophet_whole.pkl', 'rb'))\ndf = pd.DataFrame(data[0])\ndf = df[organized_labels]\ndf2 = pd.DataFrame(zscore(df, axis=0), columns=organized_labels)\ng = sns.PairGrid(df2)\ng.map(sns.scatterplot, s=2, marker='o', edgecolor=None)\n# plt.savefig('pro-scatter.pdf', dpi=300, bbox_inches='tight')\nplt.savefig('pro-scatter.png', dpi=300, bbox_inches='tight')"
  },
  {
    "objectID": "ACL/acl paper plots.html#histogram-for-christmas-carol",
    "href": "ACL/acl paper plots.html#histogram-for-christmas-carol",
    "title": "Notebook to generate plots for the ACL Paper",
    "section": "histogram for christmas carol",
    "text": "histogram for christmas carol\n\nfiles\n\n['/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/1a christmas carol_cleaned_dcltr_base.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/2a christmas carol_cleaned_dcltr_sm.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/3a christmas carol_cleaned_if_FT.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/4a christmas carol_cleaned_if_glove.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/5a christmas carol_cleaned_distil.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/6a christmas carol_cleaned_roberta.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/7a christmas carol_cleaned_use.npy',\n '/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol/8a christmas carol_cleaned_lexical_wt_ssm.npy']\n\n\n\nfig, ax = plt.subplots(4, 2, figsize=(4,6), sharex=True, sharey=True)\nk = 0\nfor row in range(4):\n    for col in range(2):\n        # ax[row][col].set_title(df.columns[k])\n        x = zscore(normalize(df2[organized_labels[k]]))\n        # x = zscore(normalize(df['RoBERTa']))\n        sns.histplot(x, bins=7, ax=ax[row][col], binwidth=1)\n        ax[row][col].set_title(label2[k])\n        # ax[row][col].set_xlabel('RB')\n        k += 1\n        del x\nplt.tight_layout()\n# plt.savefig('histogram.pdf', dpi=300, bbox_inches='tight')\n\n\n\n\n\ndf2 = pd.DataFrame(d)"
  },
  {
    "objectID": "ACL/acl paper plots.html#standardized-ssms",
    "href": "ACL/acl paper plots.html#standardized-ssms",
    "title": "Notebook to generate plots for the ACL Paper",
    "section": "Standardized SSMs",
    "text": "Standardized SSMs\n\nfc = globtastic('/content/drive/MyDrive/AAA_Thesis/final/novels/a_christmas_carol', file_glob='*.npy')\n\n\ncc = Path('/content/cc')\ncc.mkdir()\n\n\nfor f in fc:\n    f = Path(f)\n    fname = f.stem.split('_cleaned_')\n    book, method = fname[0], label(fname[1])\n            \n    title = f'{book.title()} {method}'\n    \n    em = np.load(f)\n    # plt.subplot(4, 2, i + 1 )\n    # if start == 0:\n    #     start = 1\n    \n    # if end == -1:\n    #     end = len(em)\n        \n        \n    # ticks = np.linspace(1, len(em), 5, dtype=int)\n    # labels = np.linspace(1, len(em), 5, dtype=int)\n\n    if fname[1] == 'lexical_wt_ssm':\n        sim = em\n        # print(em.shape)\n        n = normalize(sim)\n        np.fill_diagonal(sim, 1)\n    else:\n        sim = cosine_similarity(em, em)\n        n = normalize(sim)\n    \n    numerator = n - np.mean(n)\n    denominator = np.sqrt(np.sum(numerator**2) / (numerator.size - 1) )\n\n    ab1 = numerator / denominator"
  },
  {
    "objectID": "AAAI/master_nb.html",
    "href": "AAAI/master_nb.html",
    "title": "Master notebook",
    "section": "",
    "text": "!pip install colab_ssh --upgrade\nfrom colab_ssh import launch_ssh_cloudflared, init_git_cloudflared\nlaunch_ssh_cloudflared(password=\"mypassword\")\n\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting colab_ssh\n  Downloading colab_ssh-0.3.27-py3-none-any.whl (26 kB)\nInstalling collected packages: colab-ssh\nSuccessfully installed colab-ssh-0.3.27\n\n\n\n\n    \n        \n    \n    Don't worry, you only have to do this once per client machine.\n    \n        Download Cloudflared (Argo Tunnel), then copy the absolute path of the cloudflare binary\n        Now, you have to append the following to your SSH config file (usually under ~/.ssh/config), and make sure you replace the placeholder with the path you copied in Step 1:\n    \n    Host *.trycloudflare.com\n    HostName %h\n    User root\n    Port 22\n    ProxyCommand <PUT_THE_ABSOLUTE_CLOUDFLARE_PATH_HERE> access ssh --hostname %h\n    \n\n\n    \n        \n        To connect using your terminal, type this command:\n        ssh cj-declined-viewers-products.trycloudflare.com\n    \n    \n        \n        You can also connect with VSCode Remote SSH (Ctrl+Shift+P and type \"Connect to Host...\"). Then, paste the following hostname in the opened command palette:\n        cj-declined-viewers-products.trycloudflare.com"
  },
  {
    "objectID": "AAAI/master_nb.html#models",
    "href": "AAAI/master_nb.html#models",
    "title": "Master notebook",
    "section": "Models",
    "text": "Models\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nMounted at /content/drive\n\n\n\n!git clone https://github.com/facebookresearch/InferSent.git\n\n!pip install -U sentence-transformers --quiet\nfrom sentence_transformers import SentenceTransformer\nmodel_distil = SentenceTransformer('distilbert-base-nli-mean-tokens', device='cuda')\nmodel_robeta = SentenceTransformer('roberta-large-nli-stsb-mean-tokens', device='cuda')\nmodel_mpnet = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device='cuda')\nsmall = SentenceTransformer(\"johngiorgi/declutr-small\", device='cuda')\nbase = SentenceTransformer(\"johngiorgi/declutr-base\", device='cuda')\n\nimport tensorflow_hub as hub\nmodel_use = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n\nCloning into 'InferSent'...\nremote: Enumerating objects: 259, done.\nremote: Total 259 (delta 0), reused 0 (delta 0), pack-reused 259\nReceiving objects: 100% (259/259), 424.15 KiB | 7.44 MiB/s, done.\nResolving deltas: 100% (135/135), done.\n     |████████████████████████████████| 85 kB 3.1 MB/s \n     |████████████████████████████████| 4.7 MB 31.6 MB/s \n     |████████████████████████████████| 1.2 MB 53.1 MB/s \n     |████████████████████████████████| 101 kB 12.7 MB/s \n     |████████████████████████████████| 596 kB 60.9 MB/s \n     |████████████████████████████████| 6.6 MB 54.8 MB/s \n  Building wheel for sentence-transformers (setup.py) ... done\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo sentence-transformers model found with name /root/.cache/torch/sentence_transformers/johngiorgi_declutr-small. Creating a new one with MEAN pooling.\nSome weights of the model checkpoint at /root/.cache/torch/sentence_transformers/johngiorgi_declutr-small were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo sentence-transformers model found with name /root/.cache/torch/sentence_transformers/johngiorgi_declutr-base. Creating a new one with MEAN pooling.\nSome weights of the model checkpoint at /root/.cache/torch/sentence_transformers/johngiorgi_declutr-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.bias']\n- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n\n\n\ndef small_embeddings(sentences, fname):\n    embeddings = np.asarray(small(sentences, batch_size=128))\n    temp = f'{fname.stem}_dcltr_sm.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\ndef base_embeddings(sentences, fname):\n    embeddings = np.asarray(base(sentences, batch_size=128))\n    temp = f'{fname.stem}_dcltr_base.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\n\nimport numpy as np\nimport torch\nfrom InferSent import models\nimport InferSent\ndef if_glove(sentences, fname):\n    # from models import InferSent\n    MODEL_PATH = '/content/drive/MyDrive/infersent/encoder/infersent1.pkl'\n    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n                    'pool_type': 'max', 'dpout_model': 0.0, 'version': 1}\n    model = models.InferSent(params_model)\n    model.load_state_dict(torch.load(MODEL_PATH))\n\n    # Keep it on CPU or put it on GPU\n    use_cuda = True\n    model_glove = model.cuda() if use_cuda else model\n    \n    W2V_PATH = '/content/drive/MyDrive/infersent/glove.840B.300d.txt'\n    model_glove.set_w2v_path(W2V_PATH)\n    model_glove.build_vocab_k_words(K=100000)\n    embeddings = model_glove.encode(sentences, bsize=64, tokenize=False, verbose=True )\n    temp = f'{fname.stem}_if_glove.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\ndef if_ft(sentences, fname):\n    MODEL_PATH = '/content/drive/MyDrive/infersent/encoder/infersent2.pkl'\n    params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n                    'pool_type': 'max', 'dpout_model': 0.0, 'version': 2}\n    model = models.InferSent(params_model)\n    model.load_state_dict(torch.load(MODEL_PATH))\n\n    # Keep it on CPU or put it on GPU\n    use_cuda = True\n    model_fasttext = model.cuda() if use_cuda else model\n    # If infersent1 -> use GloVe embeddings. If infersent2 -> use InferSent embeddings.\n    W2V_PATH = '/content/drive/MyDrive/infersent/crawl-300d-2M.vec'\n    model_fasttext.set_w2v_path(W2V_PATH)\n    model_fasttext.build_vocab_k_words(K=100000)\n    embeddings = model_fasttext.encode(sentences, bsize=64, tokenize=False, verbose=True )\n    temp = f'{fname.stem}_if_FT.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\n\ndef sent_trans(sentences, fname):\n    embeddings = np.asarray(model_distil.encode(sentences, batch_size=128 ))\n    temp = f'{fname.stem}_distil.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\n    f2 = np.asarray(model_robeta.encode(sentences, batch_size=128))\n    temp = f'{fname.stem}_roberta.npy'\n    np.save(temp, f2)\n    del f2\n\n    f2 = np.asarray(model_mpnet.encode(sentences, batch_size=128))\n    temp = f'{fname.stem}_mpnet.npy'\n    np.save(temp, f2)\n    del f2\n\n    embeddings = np.asarray(small.encode(sentences, batch_size=128))\n    temp = f'{fname.stem}_dcltr_sm.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\n    embeddings = np.asarray(base.encode(sentences, batch_size=128))\n    temp = f'{fname.stem}_dcltr_base.npy'\n    np.save(temp, embeddings)\n    del embeddings\n\ndef use_embed(sentences, fname):\n    final = np.asarray(model_use(sentences))\n    temp = f'{fname.stem}_use.npy'\n    np.save(temp, final)\n    del final\n\n\n!pip install git+https://github.com/deven367/clean_plot.git -q\n\n     |█▍                              | 10 kB 25.4 MB/s eta 0:00:01     |██▉                             | 20 kB 8.4 MB/s eta 0:00:01     |████▏                           | 30 kB 3.6 MB/s eta 0:00:01     |█████▋                          | 40 kB 4.6 MB/s eta 0:00:01     |███████                         | 51 kB 4.3 MB/s eta 0:00:01     |████████▍                       | 61 kB 5.1 MB/s eta 0:00:01     |█████████▊                      | 71 kB 5.5 MB/s eta 0:00:01     |███████████▏                    | 81 kB 4.3 MB/s eta 0:00:01     |████████████▌                   | 92 kB 4.8 MB/s eta 0:00:01     |██████████████                  | 102 kB 3.8 MB/s eta 0:00:01     |███████████████▎                | 112 kB 3.8 MB/s eta 0:00:01     |████████████████▊               | 122 kB 3.8 MB/s eta 0:00:01     |██████████████████              | 133 kB 3.8 MB/s eta 0:00:01     |███████████████████▌            | 143 kB 3.8 MB/s eta 0:00:01     |████████████████████▉           | 153 kB 3.8 MB/s eta 0:00:01     |██████████████████████▎         | 163 kB 3.8 MB/s eta 0:00:01     |███████████████████████▋        | 174 kB 3.8 MB/s eta 0:00:01     |█████████████████████████       | 184 kB 3.8 MB/s eta 0:00:01     |██████████████████████████▍     | 194 kB 3.8 MB/s eta 0:00:01     |███████████████████████████▉    | 204 kB 3.8 MB/s eta 0:00:01     |█████████████████████████████▏  | 215 kB 3.8 MB/s eta 0:00:01     |██████████████████████████████▋ | 225 kB 3.8 MB/s eta 0:00:01     |████████████████████████████████| 235 kB 3.8 MB/s eta 0:00:01     |████████████████████████████████| 235 kB 3.8 MB/s \n  Building wheel for clean-plot (setup.py) ... done\n\n\n\nfrom pathlib import Path\nfrom clean_plot.core import get_data\nfrom clean_plot.functions import split_by_newline\nfrom fastcore.xtras import globtastic\n\n\nfiles = globtastic('/content/txt', file_glob='*_cleaned.txt', recursive=False)\nfiles\n\n(#1) ['/content/txt/heart of darkness_cleaned.txt']\n\n\n\ndef generate_all(sentences, fname):\n    if_glove(sentences, fname)\n    if_ft(sentences, fname)\n\n    sent_trans(sentences, fname)\n\n    use_embed(sentences, fname)\n\n\nimport os\n\n\nfor f in files:\n    f = Path(f)\n    print(f'working on {f.stem}')\n    data = split_by_newline(get_data(f))\n    new_path = Path(f'/content/{f.stem}')\n    new_path.mkdir(exist_ok = True)\n    os.chdir(new_path)\n    generate_all(data, f)\n    print('-'*45)\n\nworking on heart of darkness_cleaned\nVocab size : 100000\nNb words kept : 35491/42770 (83.0%)\n\n\n/content/InferSent/models.py:207: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n  sentences = np.array(sentences)[idx_sort]\n\n\nSpeed : 1174.6 sentences/s (gpu mode, bsize=64)\nVocab size : 100000\nNb words kept : 36156/42770 (84.5%)\nSpeed : 2227.6 sentences/s (gpu mode, bsize=64)\n---------------------------------------------\n\n\n\n!cp -r /content/Laughter_An_Essay_on_the_Meaning_of_the_Comic_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n\n\n!cp -r /content/A_Modest_Proposal_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/A_Study_in_Scarlet_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/Adventures_of_Huckleberry_Finn_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/Dragons_and_Cherry_Blossoms_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/Little_Women_by_Louisa_May_Alcott_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n\n!cp -r /content/Ruth_of_the_USA_by_Edwin_Balmer_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/The_Catspaw_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/The_Hound_of_the_Baskervilles_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/The_Scarlet_Letter_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/The_Sons_of_Japheth_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n\n\n!cp -r /content/picture_of_dorian_gray_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/siddartha_cleaned/ /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/metamorphosis_cleaned /content/drive/MyDrive/AAA_Thesis/\n!cp -r /content/a_christmas_carol_cleaned /content/drive/MyDrive/AAA_Thesis\n!cp -r /content/the_prophet_cleaned /content/drive/MyDrive/AAA_Thesis\n\n\n!cp -r /content/heart_of_darkness_cleaned /content/drive/MyDrive/AAA_Thesis\n\n\n!nvidia-smi\n\nSun Jul  3 20:23:50 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   54C    P0    28W /  70W |   8878MiB / 15109MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n+-----------------------------------------------------------------------------+\n\n\n\n!du -sh /content/drive/MyDrive/AAA_Thesis/A_Modest_Proposal/\n\n15M /content/drive/MyDrive/AAA_Thesis/A_Modest_Proposal/"
  },
  {
    "objectID": "AAAI/supp_declutr_aaai.html",
    "href": "AAAI/supp_declutr_aaai.html",
    "title": "AAAI",
    "section": "",
    "text": "# https://github.com/JohnGiorgi/DeCLUTR\n\n!pip install git+https://github.com/JohnGiorgi/DeCLUTR.git --quiet\n!pip install git+https://github.com/deven367/clean_plot --q\n\n  Installing build dependencies ... done\n  Getting requirements to build wheel ... done\n    Preparing wheel metadata ... done\n     |████████████████████████████████| 235 kB 5.1 MB/s \n  Building wheel for clean-plot (setup.py) ... done\nFinally, let’s check to see if we have a GPU available, which we can use to dramatically speed up the embedding of text"
  },
  {
    "objectID": "AAAI/supp_declutr_aaai.html#as-a-library",
    "href": "AAAI/supp_declutr_aaai.html#as-a-library",
    "title": "AAAI",
    "section": "1️⃣ As a library",
    "text": "1️⃣ As a library\nTo use the model as a library, import Encoder and pass it some text (it accepts both strings and lists of strings)\n\nfrom declutr import Encoder\nimport numpy as np\n# This can be a path on disk to a model you have trained yourself OR\n# the name of one of our pretrained models.\n\n# downloading the model for declutr bas\nencoder_base = Encoder(\"declutr-base\", cuda_device = cuda_device)\n\ndownloading: 100%|##########| 465705314/465705314 [00:32<00:00, 14400594.43B/s]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWARNING:transformers.modeling_utils:Some weights of RobertaForMaskedLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['lm_head.decoder.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nfrom clean_plot.utils import *\nfrom fastcore.xtras import globtastic\nfrom pathlib import Path\n\n\ndef base_embeddings(fname):\n    embeddings = encoder_base(split_by_newline(get_data(fname)), batch_size=32)\n    temp = Path(fname).stem +'_dcltr_base.npy'\n    np.save(temp, embeddings)\n    del temp\n\n\nfiles = globtastic('/content/txt', file_glob='*.txt')\n\nUpload a txt folder containing all the corpora\n\nfor f in files:\n    base_embeddings(f)"
  }
]