{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# distil-roberta\n",
    "> Notebook for distil bert and Roberta\n",
    "\n",
    "- skip_exec: true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 81kB 6.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.3MB 12.5MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.2MB 46.6MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.3MB 54.4MB/s \n",
      "\u001b[K     |████████████████████████████████| 901kB 42.6MB/s \n",
      "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "# download the sentence transformers library\n",
    "!pip install -U sentence-transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4790798d0b724f0285fd41d9d6a79e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=244733649.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download the encoder for distilbert and load the encoder on gpu, if available\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model_distil = SentenceTransformer('distilbert-base-nli-mean-tokens', device='cuda')\n",
    "\n",
    "# use this if GPU is unavailable\n",
    "# model_distil = SentenceTransformer('distilbert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d067cede024e30a8d3829742810edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1313952051.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download the encoder for RoBERTa\n",
    "model_robeta = SentenceTransformer('roberta-large-nli-stsb-mean-tokens', device='cuda')\n",
    "\n",
    "# use this if GPU is unavailable\n",
    "# model_robeta = SentenceTransformer('roberta-large-nli-stsb-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# function to get sentences from the cleaned txt files\n",
    "def get_sentenes(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        all = f.read()\n",
    "        f.close()\n",
    "    name = fname[:-4]\n",
    "    all_sentences = [line for line in all.split('\\n') if len(line) > 0]\n",
    "    print('{} contains {} sentences'.format(name, len(all_sentences)))\n",
    "    return all_sentences\n",
    "\n",
    "# function to generate embeddings for distilbert and roberta, \n",
    "# save them as .npy arrays\n",
    "def generate(fname):\n",
    "    sent = get_sentenes(fname)\n",
    "    final = np.asarray(model_distil.encode(sent))\n",
    "    np.save(fname[:-4]+'_distil.npy',final)\n",
    "\n",
    "    f2 = np.asarray(model_robeta.encode(sent))\n",
    "    np.save(fname[:-4]+'_roberta.npy',f2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a christmas carol_cleaned contains 1942 sentences\n",
      "heart of darkness_cleaned contains 2430 sentences\n",
      "metamorphosis_cleaned contains 795 sentences\n",
      "the prophet_cleaned contains 647 sentences\n"
     ]
    }
   ],
   "source": [
    "# generate embeddings for all the datasets, make sure the .txt files are in the correct directory\n",
    "\n",
    "fname = 'a christmas carol_cleaned.txt'\n",
    "generate(fname)\n",
    "\n",
    "fname = 'heart of darkness_cleaned.txt'\n",
    "generate(fname)\n",
    "\n",
    "fname = 'metamorphosis_cleaned.txt'\n",
    "generate(fname)\n",
    "\n",
    "fname = 'the prophet_cleaned.txt'\n",
    "generate(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
